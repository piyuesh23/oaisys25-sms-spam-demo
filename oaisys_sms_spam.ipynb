{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df9cf8af"
      },
      "source": [
        "# Task\n",
        "Fine-tune a DistilGPT2 model for SMS spam detection using the `uciml/sms-spam-collection-dataset` from Kaggle, and then demonstrate its inference capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fcc9671"
      },
      "source": [
        "## Setup and Data Loading\n",
        "\n",
        "### Subtask:\n",
        "Install necessary libraries (transformers, datasets, pandas, scikit-learn) and download/load the uciml/sms-spam-collection-dataset from Kaggle. Display the first few rows and basic statistics of the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79fa15c5"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I will install all the necessary libraries for the subtask, including transformers, datasets, pandas, scikit-learn, and kaggle.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a3f8340",
        "outputId": "b1bd10ee-18ab-4477-9e39-7e84282a67ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers datasets pandas scikit-learn kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a0f1f40"
      },
      "source": [
        "## Configure Kaggle API with Environment Variables\n",
        "\n",
        "### Subtask:\n",
        "Set up Kaggle API credentials by uploading `kaggle.json` and configuring environment variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a782e92a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = 'piyueshkumar'\n",
        "os.environ['KAGGLE_KEY'] = '91ec945b09ef34b522f2dbb98a55489e'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97123bbf"
      },
      "source": [
        "## Download and Unzip Dataset\n",
        "\n",
        "### Subtask:\n",
        "Download the `uciml/sms-spam-collection-dataset` from Kaggle using the `kaggle` CLI (which will now use the environment variables for authentication) and unzip the downloaded file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e81c6cc6",
        "outputId": "047b1f8a-8524-4390-a44b-cad4c34e90c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset\n",
            "License(s): unknown\n",
            "Downloading sms-spam-collection-dataset.zip to /content\n",
            "  0% 0.00/211k [00:00<?, ?B/s]\n",
            "100% 211k/211k [00:00<00:00, 542MB/s]\n",
            "Archive:  sms-spam-collection-dataset.zip\n",
            "  inflating: spam.csv                \n",
            "Dataset downloaded and unzipped.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Download the dataset from Kaggle\n",
        "!kaggle datasets download -d uciml/sms-spam-collection-dataset\n",
        "\n",
        "# Unzip the downloaded file, overwriting existing files without prompting\n",
        "!unzip -o sms-spam-collection-dataset.zip\n",
        "print(\"Dataset downloaded and unzipped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "044df18f",
        "outputId": "3a555ab4-32af-4302-9ca2-cd701b26a3bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 rows of the dataset:\n",
            "     v1                                                 v2 Unnamed: 2  \\\n",
            "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
            "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
            "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
            "\n",
            "  Unnamed: 3 Unnamed: 4  \n",
            "0        NaN        NaN  \n",
            "1        NaN        NaN  \n",
            "2        NaN        NaN  \n",
            "3        NaN        NaN  \n",
            "4        NaN        NaN  \n",
            "\n",
            "Basic statistics of the dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572 entries, 0 to 5571\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   v1          5572 non-null   object\n",
            " 1   v2          5572 non-null   object\n",
            " 2   Unnamed: 2  50 non-null     object\n",
            " 3   Unnamed: 3  12 non-null     object\n",
            " 4   Unnamed: 4  6 non-null      object\n",
            "dtypes: object(5)\n",
            "memory usage: 217.8+ KB\n",
            "None\n",
            "\n",
            "Descriptive statistics for numerical columns:\n",
            "          v1                      v2  \\\n",
            "count   5572                    5572   \n",
            "unique     2                    5169   \n",
            "top      ham  Sorry, I'll call later   \n",
            "freq    4825                      30   \n",
            "\n",
            "                                               Unnamed: 2  \\\n",
            "count                                                  50   \n",
            "unique                                                 43   \n",
            "top      bt not his girlfrnd... G o o d n i g h t . . .@\"   \n",
            "freq                                                    3   \n",
            "\n",
            "                   Unnamed: 3 Unnamed: 4  \n",
            "count                      12          6  \n",
            "unique                     10          5  \n",
            "top      MK17 92H. 450Ppw 16\"    GNT:-)\"  \n",
            "freq                        2          2  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas DataFrame\n",
        "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"\\nBasic statistics of the dataset:\")\n",
        "print(df.info())\n",
        "print(\"\\nDescriptive statistics for numerical columns:\")\n",
        "print(df.describe(include='all'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "930cd2c2"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Clean and preprocess the text data. Tokenize the SMS messages using the DistilGPT2 tokenizer and create numerical labels (0 for ham, 1 for spam). Split the dataset into training and validation/test sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eae54f4"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I will clean the dataframe by dropping irrelevant columns, renaming the remaining ones for clarity, and converting the categorical labels 'ham' and 'spam' into numerical format (0 and 1 respectively) as required for model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36107255",
        "outputId": "7175587c-9526-4be7-9e5a-f20a632cc15b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame after cleaning and renaming:\n",
            "   label                                               text\n",
            "0      0  Go until jurong point, crazy.. Available only ...\n",
            "1      0                      Ok lar... Joking wif u oni...\n",
            "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3      0  U dun say so early hor... U c already then say...\n",
            "4      0  Nah I don't think he goes to usf, he lives aro...\n",
            "\n",
            "Value counts for the label column:\n",
            "label\n",
            "0    4825\n",
            "1     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Drop the irrelevant columns\n",
        "df = df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])\n",
        "\n",
        "# Rename columns for clarity\n",
        "df = df.rename(columns={'v1': 'label', 'v2': 'text'})\n",
        "\n",
        "# Convert 'label' column to numerical format (0 for ham, 1 for spam)\n",
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "print(\"DataFrame after cleaning and renaming:\")\n",
        "print(df.head())\n",
        "print(\"\\nValue counts for the label column:\")\n",
        "print(df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253,
          "referenced_widgets": [
            "3b72e6ac44204146bf524e6985ae3a66",
            "62553849747f401c8d09d511864fb145",
            "77417ec395d64aa68d33a54a8aa2b570",
            "e121191377ef4d4480d0aaf436ef9fad",
            "24b7edd4cc3a4456adf7716d5d1107ac",
            "80a68807a7814a1585a6aee585e40768",
            "64af7d20096d4d87946d04cfe8bf5f2d",
            "f066b79e8d984e45993f7ae041f77b86",
            "960059f35af443bba6ce8c8480a2b3af",
            "084b70a3032c4fe7925f9b89abf6aea3",
            "59fb3f53a6af4ec297f66b4dead10302"
          ]
        },
        "id": "4a7346a4",
        "outputId": "5c6c3eb1-f6b0-4bc7-ac64-d2211bd1add3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b72e6ac44204146bf524e6985ae3a66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5572 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer loaded and preprocessing function defined.\n",
            "Dataset converted to Hugging Face Dataset format.\n",
            "Dataset split into training, validation, and test sets.\n",
            "\n",
            "Training dataset size: 4457\n",
            "Validation dataset size: 557\n",
            "Test dataset size: 558\n",
            "\n",
            "First entry of the tokenized training dataset:\n",
            "{'labels': 0, 'input_ids': [5840, 1671, 342, 492, 9937, 16998, 492, 4164, 334, 287, 610, 403, 288, 3099, 1667, 81, 469, 492, 2787, 368, 1671, 30, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the DistilGPT2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "\n",
        "# Add a padding token if not already present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
        "\n",
        "# Define a preprocessing function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "# Convert pandas DataFrame to Hugging Face Dataset\n",
        "hf_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Apply the preprocessing function\n",
        "tokenized_datasets = hf_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove the original text column and other unnecessary columns if present\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['text'])\n",
        "\n",
        "# Rename 'label' to 'labels' for compatibility with Hugging Face models\n",
        "tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
        "\n",
        "# Split the dataset into training and validation/test sets (80% train, 20% validation/test)\n",
        "train_test_split_dataset = tokenized_datasets.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# Further split the test set into validation and test sets (10% validation, 10% test)\n",
        "# This is done by splitting the 20% test_size into two halves\n",
        "validation_test_split = train_test_split_dataset['test'].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "train_dataset = train_test_split_dataset['train']\n",
        "validation_dataset = validation_test_split['train']\n",
        "test_dataset = validation_test_split['test']\n",
        "\n",
        "print(\"Tokenizer loaded and preprocessing function defined.\")\n",
        "print(\"Dataset converted to Hugging Face Dataset format.\")\n",
        "print(\"Dataset split into training, validation, and test sets.\")\n",
        "\n",
        "print(f\"\\nTraining dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(validation_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "print(\"\\nFirst entry of the tokenized training dataset:\")\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25b4d808"
      },
      "source": [
        "## Model Loading and Configuration\n",
        "\n",
        "### Subtask:\n",
        "Load the pre-trained DistilGPT2 model and its corresponding tokenizer from Hugging Face. Adapt the model for sequence classification by adding a classification head suitable for binary classification (spam/ham).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "425b31f5"
      },
      "source": [
        "**Reasoning**:\n",
        "I will load the pre-trained DistilGPT2 model for sequence classification and configure it for binary classification by setting the number of labels and the padding token ID.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "76ea549c074d4ee1bf8202ec1fa7f0af",
            "92e5e052b97447e99f835f411c06fd8b",
            "c38841ff2e8945639894fa422242ad69",
            "627e1b36f282457e8215c158671b4c48",
            "7232f81b2ef243c28294412e460ce62f",
            "63020ff023024fef8d588221843c0967",
            "a3f672a4036e4302960dc7ace2a88065",
            "a3fc2b57342f453faa88e255204dafde",
            "986ec781988f4f719c7dd818b2e99df1",
            "d65cce1019ff415c98b2f093ba8b2d5d",
            "7a6e62a6eb864323a3ef86e3bf40637b"
          ]
        },
        "id": "b8b9fa16",
        "outputId": "7e6aff23-25ed-4062-d19f-1b8005952a63"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76ea549c074d4ee1bf8202ec1fa7f0af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DistilGPT2 model loaded and configured for binary sequence classification.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Load the pre-trained DistilGPT2 model for sequence classification\n",
        "# Set num_labels to 2 for binary classification (ham/spam)\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilgpt2', num_labels=2)\n",
        "\n",
        "# Set the model's padding token ID to the tokenizer's padding token ID\n",
        "# This ensures consistency during inference and training, as DistilGPT2 doesn't have a default pad_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(\"DistilGPT2 model loaded and configured for binary sequence classification.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "434c8a8c"
      },
      "source": [
        "## Evaluation Before Fine-tuning\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the model's performance on the test set before fine-tuning to establish a baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "f55cb8a2",
        "outputId": "ca851135-6361-4aff-d28f-fd1deb087369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating baseline model performance on the test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [70/70 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moaisys\u001b[0m (\u001b[33moaisys-theoaisys\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251129_182025-z340r02t</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oaisys-theoaisys/huggingface/runs/z340r02t' target=\"_blank\">fragrant-violet-2</a></strong> to <a href='https://wandb.ai/oaisys-theoaisys/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oaisys-theoaisys/huggingface' target=\"_blank\">https://wandb.ai/oaisys-theoaisys/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oaisys-theoaisys/huggingface/runs/z340r02t' target=\"_blank\">https://wandb.ai/oaisys-theoaisys/huggingface/runs/z340r02t</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Model Evaluation Results:\n",
            "{'eval_loss': 0.9012208580970764, 'eval_model_preparation_time': 0.0022, 'eval_accuracy': 0.8566308243727598, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 4.3208, 'eval_samples_per_second': 129.144, 'eval_steps_per_second': 16.201}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# 2. Define a function named compute_metrics\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    predictions = pred.predictions.argmax(-1)\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# 4. Initialize TrainingArguments for evaluation\n",
        "# We only need output_dir and per_device_eval_batch_size for evaluation here\n",
        "eval_args = TrainingArguments(\n",
        "    output_dir='./baseline_results',\n",
        "    per_device_eval_batch_size=8,\n",
        "    # Other training arguments like num_train_epochs, learning_rate etc. are not needed for a simple evaluation\n",
        ")\n",
        "\n",
        "# 6. Initialize a Trainer instance\n",
        "baseline_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=eval_args,\n",
        "    eval_dataset=test_dataset, # Evaluate on the test dataset\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 7. Call trainer.evaluate() to get the baseline evaluation results and print them\n",
        "print(\"\\nEvaluating baseline model performance on the test set...\")\n",
        "baseline_metrics = baseline_trainer.evaluate()\n",
        "print(\"Baseline Model Evaluation Results:\")\n",
        "print(baseline_metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ac64dcb7",
        "outputId": "46a8cc88-8775-4b88-9c8c-3b99e5b2b164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting model training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='837' max='837' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [837/837 03:35, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.080500</td>\n",
              "      <td>0.018418</td>\n",
              "      <td>0.992819</td>\n",
              "      <td>0.968254</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.938462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.039922</td>\n",
              "      <td>0.992819</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>0.984127</td>\n",
              "      <td>0.953846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.048600</td>\n",
              "      <td>0.013784</td>\n",
              "      <td>0.996409</td>\n",
              "      <td>0.984615</td>\n",
              "      <td>0.984615</td>\n",
              "      <td>0.984615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model training complete.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "\n",
        "# 1. Define TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,               # Total number of training epochs\n",
        "    per_device_train_batch_size=16,   # Batch size per device during training\n",
        "    per_device_eval_batch_size=16,    # Batch size for evaluation\n",
        "    warmup_steps=500,                 # Number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,                # Strength of weight decay\n",
        "    logging_dir='./logs',             # Directory for storing logs\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",          # Evaluate the model at the end of each epoch\n",
        "    save_strategy=\"epoch\",            # Save the model checkpoint at the end of each epoch\n",
        "    load_best_model_at_end=True,      # Load the best model after training\n",
        "    metric_for_best_model=\"f1\",       # Use F1 score to determine the best model\n",
        "    report_to=\"none\" # Disable integration with Weights & Biases\n",
        ")\n",
        "\n",
        "# Data collator for dynamic padding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# 2. Initialize a Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset, # Use validation_dataset for evaluation during training\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 3. Train the model\n",
        "print(\"Starting model training...\")\n",
        "trainer.train()\n",
        "print(\"Model training complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a780607"
      },
      "source": [
        "## Demonstrate Inference\n",
        "\n",
        "### Subtask:\n",
        "Fine-tuned model to predict whether new, unseen SMS messages are 'ham' or 'spam'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43d229f6"
      },
      "source": [
        "**Reasoning**:\n",
        "The model has been fine-tuned. The next step is to evaluate its performance on the test set and compare these metrics (accuracy, precision, recall, F1-score) with the `baseline_metrics` obtained before fine-tuning to demonstrate the improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddc40a0b",
        "outputId": "46dab06a-8532-4980-ec19-e309eea7b1bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Inference Examples ---\n",
            "SMS: \"Congratulations! You've won a \\$1,000 gift card! Claim your prize now.\"\n",
            "Prediction: spam (Probability: 0.9994)\n",
            "\n",
            "SMS: \"Hey, just wanted to check in and see how you're doing. Long time no talk!\"\n",
            "Prediction: ham (Probability: 1.0000)\n",
            "\n",
            "SMS: \"URGENT! Your mobile number has been selected for a free prize. Call 09061701461 now!\"\n",
            "Prediction: spam (Probability: 1.0000)\n",
            "\n",
            "SMS: \"Can we meet tomorrow for lunch?\"\n",
            "Prediction: ham (Probability: 0.9998)\n",
            "\n",
            "SMS: \"WINNER! Your account has been credited with 1000 bonus points. Visit link to activate.\"\n",
            "Prediction: spam (Probability: 0.9999)\n",
            "\n",
            "SMS: \"Hi, how are you?\"\n",
            "Prediction: ham (Probability: 0.9999)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:18: SyntaxWarning: invalid escape sequence '\\$'\n",
            "<>:18: SyntaxWarning: invalid escape sequence '\\$'\n",
            "/tmp/ipython-input-2896514054.py:18: SyntaxWarning: invalid escape sequence '\\$'\n",
            "  \"Congratulations! You've won a \\$1,000 gift card! Claim your prize now.\",\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Function to predict if an SMS is spam or ham\n",
        "def predict_spam(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
        "    # Move input tensors to the same device as the model\n",
        "    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "\n",
        "    return \"spam\" if predicted_class == 1 else \"ham\", probabilities[0][predicted_class].item()\n",
        "\n",
        "# Test cases\n",
        "test_messages = [\n",
        "    \"Congratulations! You've won a \\$1,000 gift card! Claim your prize now.\",\n",
        "    \"Hey, just wanted to check in and see how you're doing. Long time no talk!\",\n",
        "    \"URGENT! Your mobile number has been selected for a free prize. Call 09061701461 now!\",\n",
        "    \"Can we meet tomorrow for lunch?\",\n",
        "    \"WINNER! Your account has been credited with 1000 bonus points. Visit link to activate.\",\n",
        "    \"Hi, how are you?\",\n",
        "]\n",
        "\n",
        "print(\"\\n--- Inference Examples ---\")\n",
        "for message in test_messages:\n",
        "    prediction, probability = predict_spam(message)\n",
        "    print(f\"SMS: \\\"{message}\\\"\")\n",
        "    print(f\"Prediction: {prediction} (Probability: {probability:.4f})\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58c45e67"
      },
      "source": [
        "## Demonstrate Inference (Before Fine-tuning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b82a3afe"
      },
      "source": [
        "## Demonstrate Inference (True Baseline Model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3a872e2"
      },
      "source": [
        "**Reasoning**:\n",
        "To correct the previous misleading baseline inference, I will now explicitly load a fresh instance of the DistilGPT2 model that has *not* been fine-tuned. This `unfine_tuned_model` will be used for demonstrating inference of the true baseline, allowing for a proper comparison with the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "824cd185",
        "outputId": "0b3bb079-bd32-478a-cc70-7ce1a2cee81e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Un-fine-tuned DistilGPT2 model loaded for baseline inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Load a fresh instance of the pre-trained DistilGPT2 model for baseline inference\n",
        "# This model has not been fine-tuned on the SMS spam dataset\n",
        "unfine_tuned_model = AutoModelForSequenceClassification.from_pretrained('distilgpt2', num_labels=2)\n",
        "\n",
        "# Set the padding token ID for consistency\n",
        "unfine_tuned_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(\"Un-fine-tuned DistilGPT2 model loaded for baseline inference.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5241340",
        "outputId": "8bc67403-4f36-4633-80ee-2e978f639369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Inference Examples (True Baseline Model) ---\n",
            "SMS: \"Congratulations! You've won a \\$1,000 gift card! Claim your prize now.\"\n",
            "Prediction: ham (Probability: 0.6944)\n",
            "\n",
            "SMS: \"Hey, just wanted to check in and see how you're doing. Long time no talk!\"\n",
            "Prediction: ham (Probability: 0.8011)\n",
            "\n",
            "SMS: \"URGENT! Your mobile number has been selected for a free prize. Call 09061701461 now!\"\n",
            "Prediction: ham (Probability: 0.7117)\n",
            "\n",
            "SMS: \"Can we meet tomorrow for lunch?\"\n",
            "Prediction: ham (Probability: 0.8468)\n",
            "\n",
            "SMS: \"WINNER! Your account has been credited with 1000 bonus points. Visit link to activate.\"\n",
            "Prediction: ham (Probability: 0.7652)\n",
            "\n",
            "SMS: \"Hi, how are you?\"\n",
            "Prediction: ham (Probability: 0.7849)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Function to predict if an SMS is spam or ham using a specified model\n",
        "def predict_spam_with_model(text, model_to_use):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
        "    # Move input tensors to the same device as the model\n",
        "    inputs = {key: value.to(model_to_use.device) for key, value in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model_to_use(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "\n",
        "    return \"spam\" if predicted_class == 1 else \"ham\", probabilities[0][predicted_class].item()\n",
        "\n",
        "print(\"\\n--- Inference Examples (True Baseline Model) ---\")\n",
        "for message in test_messages:\n",
        "    prediction, probability = predict_spam_with_model(message, unfine_tuned_model)\n",
        "    print(f\"SMS: \\\"{message}\\\"\")\n",
        "    print(f\"Prediction: {prediction} (Probability: {probability:.4f})\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee7ea36b"
      },
      "source": [
        "# Task\n",
        "Evaluate the fine-tuned DistilGPT2 model's performance on the test set, comparing metrics such as accuracy, precision, recall, and F1-score with the previously established baseline to demonstrate improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04012469"
      },
      "source": [
        "## Evaluation After Fine-tuning\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the fine-tuned model's performance on the test set. Compare these metrics (e.g., accuracy, precision, recall, F1-score) with the baseline evaluation from before fine-tuning to demonstrate improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "59636792",
        "outputId": "a18a158a-dcfd-40cb-eacd-8871f9c4a3bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating fine-tuned model performance on the test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuned Model Evaluation Results:\n",
            "{'eval_loss': 0.048322971910238266, 'eval_accuracy': 0.9910394265232975, 'eval_f1': 0.967741935483871, 'eval_precision': 1.0, 'eval_recall': 0.9375, 'eval_runtime': 2.7027, 'eval_samples_per_second': 206.461, 'eval_steps_per_second': 12.95, 'epoch': 3.0}\n",
            "\n",
            "--- Comparison with Baseline Model ---\n",
            "Baseline Model F1:  0.0\n",
            "Fine-tuned Model F1:  0.967741935483871\n",
            "Baseline Model Accuracy:  0.8566308243727598\n",
            "Fine-tuned Model Accuracy:  0.9910394265232975\n",
            "Baseline Model Precision:  0.0\n",
            "Fine-tuned Model Precision:  1.0\n",
            "Baseline Model Recall:  0.0\n",
            "Fine-tuned Model Recall:  0.9375\n",
            "\n",
            "--- Summary of Improvements ---\n",
            "F1-score improved from 0.0000 to 0.9677.\n",
            "Accuracy improved from 0.8566 to 0.9910.\n",
            "Precision improved from 0.0000 to 1.0000.\n",
            "Recall improved from 0.0000 to 0.9375.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nEvaluating fine-tuned model performance on the test set...\")\n",
        "fined_tuned_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(\"Fine-tuned Model Evaluation Results:\")\n",
        "print(fined_tuned_metrics)\n",
        "\n",
        "print(\"\\n--- Comparison with Baseline Model ---\")\n",
        "print(\"Baseline Model F1: \", baseline_metrics.get('eval_f1'))\n",
        "print(\"Fine-tuned Model F1: \", fined_tuned_metrics.get('eval_f1'))\n",
        "print(\"Baseline Model Accuracy: \", baseline_metrics.get('eval_accuracy'))\n",
        "print(\"Fine-tuned Model Accuracy: \", fined_tuned_metrics.get('eval_accuracy'))\n",
        "print(\"Baseline Model Precision: \", baseline_metrics.get('eval_precision'))\n",
        "print(\"Fine-tuned Model Precision: \", fined_tuned_metrics.get('eval_precision'))\n",
        "print(\"Baseline Model Recall: \", baseline_metrics.get('eval_recall'))\n",
        "print(\"Fine-tuned Model Recall: \", fined_tuned_metrics.get('eval_recall'))\n",
        "\n",
        "print(\"\\n--- Summary of Improvements ---\")\n",
        "if fined_tuned_metrics.get('eval_f1') > baseline_metrics.get('eval_f1'):\n",
        "    print(f\"F1-score improved from {baseline_metrics.get('eval_f1'):.4f} to {fined_tuned_metrics.get('eval_f1'):.4f}.\")\n",
        "if fined_tuned_metrics.get('eval_accuracy') > baseline_metrics.get('eval_accuracy'):\n",
        "    print(f\"Accuracy improved from {baseline_metrics.get('eval_accuracy'):.4f} to {fined_tuned_metrics.get('eval_accuracy'):.4f}.\")\n",
        "if fined_tuned_metrics.get('eval_precision') > baseline_metrics.get('eval_precision'):\n",
        "    print(f\"Precision improved from {baseline_metrics.get('eval_precision'):.4f} to {fined_tuned_metrics.get('eval_precision'):.4f}.\")\n",
        "if fined_tuned_metrics.get('eval_recall') > baseline_metrics.get('eval_recall'):\n",
        "    print(f\"Recall improved from {baseline_metrics.get('eval_recall'):.4f} to {fined_tuned_metrics.get('eval_recall'):.4f}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b07e5985"
      },
      "source": [
        "## Demonstrate Inference\n",
        "\n",
        "### Subtask:\n",
        "Show examples of how to use the fine-tuned model to predict whether new, unseen SMS messages are 'ham' or 'spam'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2e47230"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "Yes, the fine-tuned DistilGPT2 model demonstrated significant improvement in performance on the test set compared to the baseline model across all evaluated metrics.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The fine-tuning process was successfully completed for the DistilGPT2 model on the SMS spam detection dataset.\n",
        "*   **F1-score** dramatically improved from 0.0000 (baseline) to 0.9677 (fine-tuned).\n",
        "*   **Accuracy** saw a substantial increase from 0.8566 (baseline) to 0.9910 (fine-tuned).\n",
        "*   **Precision** improved significantly from 0.0000 (baseline) to 1.0000 (fine-tuned).\n",
        "*   **Recall** increased from 0.0000 (baseline) to 0.9375 (fine-tuned).\n",
        "*   The fine-tuned model exhibits high performance, particularly in minimizing false positives (perfect precision) and effectively identifying relevant instances.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The fine-tuned model is highly effective for SMS spam detection, showing near-perfect precision and high recall, suggesting it is ready for deployment or further testing in a real-world environment.\n",
        "*   Given the significant improvement, further analysis could involve examining the specific types of messages (e.g., edge cases, ambiguous texts) where the fine-tuned model still makes errors, to identify potential areas for even further refinement.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "58c45e67"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "084b70a3032c4fe7925f9b89abf6aea3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b7edd4cc3a4456adf7716d5d1107ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b72e6ac44204146bf524e6985ae3a66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62553849747f401c8d09d511864fb145",
              "IPY_MODEL_77417ec395d64aa68d33a54a8aa2b570",
              "IPY_MODEL_e121191377ef4d4480d0aaf436ef9fad"
            ],
            "layout": "IPY_MODEL_24b7edd4cc3a4456adf7716d5d1107ac"
          }
        },
        "59fb3f53a6af4ec297f66b4dead10302": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62553849747f401c8d09d511864fb145": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80a68807a7814a1585a6aee585e40768",
            "placeholder": "​",
            "style": "IPY_MODEL_64af7d20096d4d87946d04cfe8bf5f2d",
            "value": "Map: 100%"
          }
        },
        "627e1b36f282457e8215c158671b4c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d65cce1019ff415c98b2f093ba8b2d5d",
            "placeholder": "​",
            "style": "IPY_MODEL_7a6e62a6eb864323a3ef86e3bf40637b",
            "value": " 353M/353M [00:06&lt;00:00, 46.6MB/s]"
          }
        },
        "63020ff023024fef8d588221843c0967": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64af7d20096d4d87946d04cfe8bf5f2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7232f81b2ef243c28294412e460ce62f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76ea549c074d4ee1bf8202ec1fa7f0af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92e5e052b97447e99f835f411c06fd8b",
              "IPY_MODEL_c38841ff2e8945639894fa422242ad69",
              "IPY_MODEL_627e1b36f282457e8215c158671b4c48"
            ],
            "layout": "IPY_MODEL_7232f81b2ef243c28294412e460ce62f"
          }
        },
        "77417ec395d64aa68d33a54a8aa2b570": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f066b79e8d984e45993f7ae041f77b86",
            "max": 5572,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_960059f35af443bba6ce8c8480a2b3af",
            "value": 5572
          }
        },
        "7a6e62a6eb864323a3ef86e3bf40637b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80a68807a7814a1585a6aee585e40768": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92e5e052b97447e99f835f411c06fd8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63020ff023024fef8d588221843c0967",
            "placeholder": "​",
            "style": "IPY_MODEL_a3f672a4036e4302960dc7ace2a88065",
            "value": "model.safetensors: 100%"
          }
        },
        "960059f35af443bba6ce8c8480a2b3af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "986ec781988f4f719c7dd818b2e99df1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3f672a4036e4302960dc7ace2a88065": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3fc2b57342f453faa88e255204dafde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c38841ff2e8945639894fa422242ad69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3fc2b57342f453faa88e255204dafde",
            "max": 352824413,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_986ec781988f4f719c7dd818b2e99df1",
            "value": 352824413
          }
        },
        "d65cce1019ff415c98b2f093ba8b2d5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e121191377ef4d4480d0aaf436ef9fad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_084b70a3032c4fe7925f9b89abf6aea3",
            "placeholder": "​",
            "style": "IPY_MODEL_59fb3f53a6af4ec297f66b4dead10302",
            "value": " 5572/5572 [00:04&lt;00:00, 1446.82 examples/s]"
          }
        },
        "f066b79e8d984e45993f7ae041f77b86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
